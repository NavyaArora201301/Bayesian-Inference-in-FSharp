{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3458380d",
   "metadata": {},
   "source": [
    "# Taking a New Approach: Hamiltonian Monte Carlo (HMC)\n",
    "\n",
    "This notebook extends the origin discussion of Bayesian Inference that I worked for my FSharp Advent of Code - 2020 [submission](https://nbviewer.jupyter.org/github/MokoSan/FSharpAdvent_2020/blob/main/BayesianInferenceInF%23.ipynb). I have been meaning to get back to some non-Generative AI Data Science methodologies and figured I left where I started by improving on the deficiencies of the Symmetric Metropolis Hastings algorithm that was implemented in the previous post.\n",
    "\n",
    "Furthermore, back in 2020, the ecosystem of .NET Interactive Notebooks was is its infancy - currently, Polyglot Notebooks, the successor to .NET Interactive Notebooks is a mature product with a lot more users and functionality such as importing notebooks. This improvement in the tooling has made this experience a lot smoother and the process to extend the behavior simpler.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "Special thanks to Navya Arora for her contribution with the data and helping me validate the overall idea. She was eager in helping and provided some great insights during the development phase of this algorithm.\n",
    "\n",
    "## Deficiencies of Symmeteric Metropolis Hastings\n",
    "\n",
    "Before diving into the Hamiltonian Monte Carlo algorithm, it's worth reiterate and elucidating on the deficiencies of our previous approach: the Symmeteric Metropolis Hastings algorithm and large number of which can be listed but the top 3, in my opinion working with this algorithm are:\n",
    "\n",
    "### 1. Limited Exploration of Parameter Space:\n",
    "\n",
    "The Symmetric Metropolis Hastings algorithm relies heavily on the proposal distribution. If the proposal distribution is poorly tuned (e.g., too narrow or too wide), the algorithm may fail to explore the parameter space effectively, leading to slow convergence or biased results.\n",
    "\n",
    "### 2. High Dependence on Hyperparameters:\n",
    "\n",
    "The algorithm requires careful tuning of hyperparameters like the proposal distribution's step size (delta). Poorly chosen values can result in either excessive rejection of proposals or inefficient exploration.\n",
    "\n",
    "### 3. Autocorrelation in Chains:\n",
    "\n",
    "The algorithm suffers from autocorrelation, where consecutive samples are highly dependent on each other. This reduces the effective sample size and makes the posterior approximation less reliable.\n",
    "\n",
    "## An Introduction to Hamiltonian Monte Carlo\n",
    "\n",
    "HMC combines ideas from physics and numerical optimization to simulate the posterior distribution. It uses the gradient of the log-posterior to guide the sampling process, reducing random walk behavior and improving convergence.\n",
    "\n",
    "The two key components of this algorithm are __Momentum Sampling__ and __Leapfrog Integration__.\n",
    "\n",
    "### Momentum Sampling\n",
    "\n",
    "In HMC, the posterior distribution is treated as a physical system, where the parameter values (position) are associated with potential energy, and an auxiliary variable called momentum is introduced to simulate kinetic energy.\n",
    "\n",
    "Momentum is sampled from a Gaussian distribution (e.g., Normal(0, 1)).\n",
    "This auxiliary variable helps guide the exploration of the parameter space by simulating physical dynamics.\n",
    "\n",
    "Momentum sampling allows the algorithm to escape local modes and explore the posterior more efficiently.\n",
    "It reduces random walk behavior, which is common in simpler MCMC methods like Metropolis Hastings.\n",
    "Mathematical Representation:\n",
    "\n",
    "Momentum ( p ) is sampled as: $p \\sim \\mathcal{N}(0, M)$ where ( M ) is the mass matrix (often set to identity for simplicity).\n",
    "\n",
    "### Leapfrog Integration\n",
    "\n",
    "Leapfrog integration is a numerical method used to simulate Hamiltonian dynamics in HMC. It updates both the position (parameter values) and momentum iteratively, ensuring energy conservation and stability.  Leapfrog integration ensures that the Hamiltonian dynamics are simulated accurately, preserving the total energy of the system. It avoids numerical instability and ensures reversibility, which is crucial for the acceptance step in HMC.\n",
    "\n",
    "#### Steps of :\n",
    "\n",
    "**Half-Step Momentum Update**\n",
    "\n",
    "The momentum is updated using the gradient of the log-posterior: \n",
    "\n",
    "$p_{\\text{new}} = p_{\\text{current}} + \\frac{\\epsilon}{2} \\cdot \\nabla \\log p(\\theta)$ where ( $\\epsilon$ ) is the step size.\n",
    "\n",
    "**Full-Step Position Update**\n",
    "\n",
    "The position is updated using the new momentum: \n",
    "\n",
    "$\\theta_{\\text{new}} = \\theta_{\\text{current}} + \\epsilon \\cdot p_{\\text{new}}$ \n",
    "\n",
    "**Second Half-Step Momentum Update**\n",
    "\n",
    "The momentum is updated again using the gradient at the new position: \n",
    "\n",
    "$p_{\\text{final}} = p_{\\text{new}} + \\frac{\\epsilon}{2} \\cdot \\nabla \\log p(\\theta_{\\text{new}})$ \n",
    "\n",
    "## All the Steps: \n",
    "\n",
    "1. **Initialization**: Start with an initial position (parameter value).\n",
    "2. **Momentum Sampling**: Sample a momentum variable from a Gaussian distribution.\n",
    "3. **Leapfrog Integration**: Simulate the Hamiltonian dynamics using the leapfrog method to propose a new position and momentum.\n",
    "4. **Acceptance Step**: Accept or reject the proposed position based on the Hamiltonian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b38897",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>MathNet.Numerics, 5.0.0</span></li><li><span>Newtonsoft.Json, 13.0.3</span></li><li><span>XPlot.Plotly, 4.1.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!import BayesianInferenceEngine.dib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de27951a",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [],
   "source": [
    "open System\n",
    "open MathNet.Numerics.Distributions\n",
    "\n",
    "type HamiltonianMonteCarloRequest = \n",
    "    { StepSize          : float\n",
    "      LeapfrogSteps     : int\n",
    "      Iterations        : int\n",
    "      BurnInIterationsPct : float\n",
    "      Chains            : int }\n",
    "\n",
    "type HamiltonianMonteCarloResult =\n",
    "    { Chains         : seq<float seq>\n",
    "      AcceptanceRate : float }\n",
    "\n",
    "let computeGradient (model: SimpleBayesianNetworkModel) (position: float) : float =\n",
    "    // Compute the gradient of the log-posterior at the given position.\n",
    "    let priorGradient = model.GetPriorProbability position\n",
    "    let likelihoodGradient = model.GetLikelihoodProbability position\n",
    "    priorGradient + likelihoodGradient // Simplified for illustration\n",
    "\n",
    "let leapfrogIntegration (position: float) (momentum: float) (stepSize: float) (model: SimpleBayesianNetworkModel) : float * float =\n",
    "    // Perform leapfrog integration to propose new position and momentum.\n",
    "    let gradient = computeGradient model position\n",
    "    let newMomentum = momentum + (stepSize / 2.0) * gradient\n",
    "    let newPosition = position + stepSize * newMomentum\n",
    "    let updatedGradient = computeGradient model newPosition\n",
    "    let finalMomentum = newMomentum + (stepSize / 2.0) * updatedGradient\n",
    "    newPosition, finalMomentum\n",
    "\n",
    "let runHamiltonianMonteCarlo (request: HamiltonianMonteCarloRequest) (model: SimpleBayesianNetworkModel) : HamiltonianMonteCarloResult =\n",
    "    let zeroOneUniform = ContinuousUniform()\n",
    "    let mutable acceptanceCount = 0\n",
    "    let chains = \n",
    "        seq { 1 .. request.Chains }\n",
    "        |> Seq.map (fun _ ->\n",
    "            let mutable position = 0.0 // Initialize position\n",
    "            let mutable chain = []\n",
    "            for _ in 1 .. request.Iterations do\n",
    "                let momentum = Normal(0.0, 1.0).Sample() // Sample momentum\n",
    "                let proposedPosition, proposedMomentum = leapfrogIntegration position momentum request.StepSize model\n",
    "                let currentHamiltonian = -model.GetPosteriorWithoutScalingFactor position + (momentum ** 2.0) / 2.0\n",
    "                let proposedHamiltonian = -model.GetPosteriorWithoutScalingFactor proposedPosition + (proposedMomentum ** 2.0) / 2.0\n",
    "                let acceptanceRatio = Math.Exp(currentHamiltonian - proposedHamiltonian)\n",
    "                if zeroOneUniform.Sample() < acceptanceRatio then\n",
    "                    position <- proposedPosition\n",
    "                    acceptanceCount <- acceptanceCount + 1\n",
    "                chain <- chain @ [position]\n",
    "            chain |> Seq.skip (int (request.BurnInIterationsPct / 100.0 * float request.Iterations))\n",
    "        )\n",
    "    let acceptanceRate = float acceptanceCount / float (request.Iterations * request.Chains)\n",
    "    { Chains = chains; AcceptanceRate = acceptanceRate }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfcffaf",
   "metadata": {},
   "source": [
    "### Example: Normal Prior and Normal Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93eeade",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "fsharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details open=\"open\" class=\"dni-treeview\"><summary><span class=\"dni-code-hint\"><code>XPlot.Plotly.PlotlyChart</code></span></summary><div><table><thead><tr></tr></thead><tbody><tr><td>Height</td><td><div class=\"dni-plaintext\"><pre>500</pre></div></td></tr><tr><td>Id</td><td><div class=\"dni-plaintext\"><pre>0a8d304b-c017-4ee4-832d-dbb193759469</pre></div></td></tr><tr><td>PlotlySrc</td><td><div class=\"dni-plaintext\"><pre>https://cdn.plot.ly/plotly-latest.min.js</pre></div></td></tr><tr><td>Width</td><td><div class=\"dni-plaintext\"><pre>700</pre></div></td></tr></tbody></table></div></details><style>\r\n",
       ".dni-code-hint {\r\n",
       "    font-style: italic;\r\n",
       "    overflow: hidden;\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview {\r\n",
       "    white-space: nowrap;\r\n",
       "}\r\n",
       ".dni-treeview td {\r\n",
       "    vertical-align: top;\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "details.dni-treeview {\r\n",
       "    padding-left: 1em;\r\n",
       "}\r\n",
       "table td {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "table tr { \r\n",
       "    vertical-align: top; \r\n",
       "    margin: 0em 0px;\r\n",
       "}\r\n",
       "table tr td pre \r\n",
       "{ \r\n",
       "    vertical-align: top !important; \r\n",
       "    margin: 0em 0px !important;\r\n",
       "} \r\n",
       "table th {\r\n",
       "    text-align: start;\r\n",
       "}\r\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "let model = @\"x ~ Normal(μ,τ) \n",
    "              y|x ~ Normal(x,σ) : observed\"\n",
    "let parsedModel = parseModel model\n",
    "let paramList = \"{Parameters: {μ : 5, τ : 3.1622, σ : 1}, observed : [9.37,10.18,9.16,11.60,10.33]}\"\n",
    "let simpleModel = \n",
    "    SimpleBayesianNetworkModel.Construct \"Normal-Normal\" parsedModel (deserializeParameters paramList)\n",
    "\n",
    "let hmcRequest : HamiltonianMonteCarloRequest = \n",
    "    { StepSize          = 0.01\n",
    "      LeapfrogSteps     = 10\n",
    "      Iterations        = 50_000 // Sparingly choose this.\n",
    "      BurnInIterationsPct = 10.0\n",
    "      Chains            = 4 }\n",
    "\n",
    "let hmcResult = runHamiltonianMonteCarlo hmcRequest simpleModel\n",
    "\n",
    "// Visualize the first chain\n",
    "let firstChain = Seq.head hmcResult.Chains\n",
    "\n",
    "Histogram(x = firstChain)\n",
    "|> Chart.Plot\n",
    "|> Chart.WithTitle \"Normal Prior and Normal Likelihood\"\n",
    "|> Chart.WithWidth 700\n",
    "|> Chart.WithHeight 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e46e9f",
   "metadata": {},
   "source": [
    "![Ham](./Images/HMC_NN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "let model = @\"x ~ Normal(μ,τ) \n",
    "              y|x ~ Normal(x,σ) : observed\"\n",
    "let parsedModel = parseModel model\n",
    "let paramList = \"{Parameters: {μ : 5, τ : 6, σ : 1}, observed : [9.37,10.18,9.16,11.60,10.33]}\"\n",
    "let simpleModel = \n",
    "    SimpleBayesianNetworkModel.Construct \"Normal-Normal\" parsedModel (deserializeParameters paramList)\n",
    "\n",
    "let hmcRequest : HamiltonianMonteCarloRequest = \n",
    "    { StepSize          = 0.01\n",
    "      LeapfrogSteps     = 10\n",
    "      Iterations        = 50_000 // Sparingly choose this.\n",
    "      BurnInIterationsPct = 10.0\n",
    "      Chains            = 4 }\n",
    "\n",
    "let hmcResult = runHamiltonianMonteCarlo hmcRequest simpleModel\n",
    "\n",
    "// Visualize the first chain\n",
    "let firstChain = Seq.head hmcResult.Chains\n",
    "\n",
    "Histogram(x = firstChain)\n",
    "|> Chart.Plot\n",
    "|> Chart.WithTitle \"Normal Prior and Normal Likelihood\"\n",
    "|> Chart.WithWidth 700\n",
    "|> Chart.WithHeight 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e976c",
   "metadata": {},
   "source": [
    "## Disadvantages of HMC\n",
    "\n",
    "The HMC algorithm is not without its flaws and some of which are:\n",
    "\n",
    "### 1. Computational Complexity\n",
    "\n",
    "- Gradient Computation:\n",
    "HMC requires the computation of the gradient of the log-posterior at every step. For complex models, this can be computationally expensive, especially when the posterior involves high-dimensional parameter spaces or intricate likelihood functions.\n",
    "\n",
    "- Leapfrog Integration:\n",
    "The iterative nature of leapfrog integration adds additional computational overhead compared to simpler MCMC methods like Metropolis Hastings.\n",
    "\n",
    "### 2. Sensitivity to Hyperparameters\n",
    "\n",
    "- Step Size $\\epsilon$:\n",
    "The step size must be carefully tuned. If it is too small, the algorithm takes many small steps, increasing runtime. If it is too large, the leapfrog integration becomes unstable, leading to poor sampling.\n",
    "\n",
    "- Number of Leapfrog Steps:\n",
    "The number of leapfrog steps must also be tuned. Too few steps result in insufficient exploration, while too many steps increase computational cost without significant improvement in sampling.\n",
    "\n",
    "### 3. Difficulty in High-Dimensional Spaces\n",
    "\n",
    "- Mass Matrix:\n",
    "In high-dimensional parameter spaces, the choice of the mass matrix (used in momentum sampling) becomes critical. Using a simple identity matrix may lead to inefficient exploration, while estimating a more complex mass matrix adds computational overhead.\n",
    "\n",
    "- Curvature:\n",
    "HMC struggles with posterior distributions that have regions of varying curvature (e.g., sharp peaks and flat valleys). This can lead to inefficient sampling or instability.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) represents a significant advancement in Bayesian inference, offering a powerful alternative to traditional MCMC methods like Metropolis-Hastings. By leveraging concepts from physics—specifically, Hamiltonian dynamics—HMC enables more efficient exploration of complex posterior landscapes, reducing autocorrelation and improving convergence rates. While it introduces additional computational complexity and requires careful tuning of hyperparameters, the benefits in terms of sampling quality and scalability are substantial, especially for high-dimensional models.\n",
    "\n",
    "Through practical examples, we've seen how HMC can be applied to both synthetic and real-world data, providing robust posterior estimates even in challenging scenarios. As probabilistic modeling continues to grow in importance across scientific and industrial domains, mastering advanced inference techniques like HMC will be invaluable for practitioners seeking both accuracy and efficiency in their analyses.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
